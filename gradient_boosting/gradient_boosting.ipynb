{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mkkEcZONBTP3",
        "outputId": "73055a1f-34e2-4052-f8e7-df5005dffd8b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pymorphy2\n",
            "  Downloading pymorphy2-0.9.1-py3-none-any.whl (55 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.5/55.5 kB\u001b[0m \u001b[31m813.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dawg-python>=0.7.1 (from pymorphy2)\n",
            "  Downloading DAWG_Python-0.7.2-py2.py3-none-any.whl (11 kB)\n",
            "Collecting pymorphy2-dicts-ru<3.0,>=2.4 (from pymorphy2)\n",
            "  Downloading pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl (8.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docopt>=0.6 (from pymorphy2)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: docopt\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=903bc72b16b5830bb1e062312600aec71028a5eddcb469e6e160da7122c55522\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
            "Successfully built docopt\n",
            "Installing collected packages: pymorphy2-dicts-ru, docopt, dawg-python, pymorphy2\n",
            "Successfully installed dawg-python-0.7.2 docopt-0.6.2 pymorphy2-0.9.1 pymorphy2-dicts-ru-2.4.417127.4579844\n"
          ]
        }
      ],
      "source": [
        "!pip install pymorphy2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import string\n",
        "import re\n",
        "import nltk\n",
        "import pymorphy2\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.metrics import precision_recall_fscore_support as score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer"
      ],
      "metadata": {
        "id": "ErubAXqDCjo3"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.set_option('display.max_colwidth', 100)"
      ],
      "metadata": {
        "id": "MG3MvWMgC65n"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Предобработка"
      ],
      "metadata": {
        "id": "0ZTxvisGC9f8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_punctuation(text):\n",
        "    text_nopunct = \"\".join([char for char in text if char not in string.punctuation])\n",
        "    return text_nopunct\n",
        "\n",
        "def tokenise(text):\n",
        "    tokens = re.split('\\W+', text)\n",
        "    return tokens\n",
        "\n",
        "def remove_stopwords(tokenised_list):\n",
        "    stopwords = nltk.corpus.stopwords.words('russian')\n",
        "    filtered_text = [word for word in tokenised_list if word not in stopwords]\n",
        "    return filtered_text\n",
        "\n",
        "def stemming(tokenised_text):\n",
        "    ps = nltk.SnowballStemmer('russian')\n",
        "    processed_text = [ps.stem(word) for word in tokenised_text]\n",
        "    return processed_text\n",
        "\n",
        "def lemmatizing(tokenized_text):\n",
        "    ma = pymorphy2.MorphAnalyzer()\n",
        "    processed_text = [ma.parse(word)[0].normal_form for word in tokenized_text]\n",
        "    return processed_text"
      ],
      "metadata": {
        "id": "JTGdUvDQDBe5"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    text = remove_punctuation(text)\n",
        "    tokens = tokenise(text)\n",
        "    processed_text = lemmatizing(tokens)\n",
        "    return processed_text"
      ],
      "metadata": {
        "id": "nS-vl3sMGFxm"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Загрузка данных"
      ],
      "metadata": {
        "id": "tGBQ8q_FDS4C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(\"requests_gpt.csv\", encoding='windows-1251', sep = ';')\n",
        "data.columns = ['label', 'requests']"
      ],
      "metadata": {
        "id": "1yKGWBMdDFQ3"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###TF-IDF для векторизации"
      ],
      "metadata": {
        "id": "2A0_Mu7qEBY9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_vect = TfidfVectorizer(analyzer=clean_text)\n",
        "\n",
        "X_tfidf = tfidf_vect.fit_transform(data['requests'])\n",
        "X_features = pd.DataFrame(X_tfidf.toarray())\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_features, data['label'], test_size = 0.3)"
      ],
      "metadata": {
        "id": "-vCbUAh6EBv_"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###GradientBoostingClassifier"
      ],
      "metadata": {
        "id": "hhX5p8arDIZs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Проверка на лучшее сочетание параметров метода GradientBoosting"
      ],
      "metadata": {
        "id": "uM0nOrU8EyWP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_GB (n_est, depth):\n",
        "  gb = GradientBoostingClassifier(n_estimators=n_est, max_depth=depth)\n",
        "  gb_model = gb.fit(X_train, y_train)\n",
        "  y_pred = gb_model.predict(X_test)\n",
        "\n",
        "  precision, recall, fscore, support = score(y_test, y_pred)\n",
        "  print('Est: {} / Depth: {} ----- Precision: {} / Recall: {} / Accuracy: {}'.format(\n",
        "        n_est, depth,\n",
        "        np.round(precision, 3), np.round(recall,3), round((y_pred==y_test).sum() / len(y_pred), 3)))"
      ],
      "metadata": {
        "id": "dJfQH9xOEoon"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for n_est in [10, 50, 100]:\n",
        "    for depth in [10, 20, 30, None]:\n",
        "        test_GB(n_est, depth)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-OVjFUgqF6Rc",
        "outputId": "89662485-7077-44d7-8435-9e68325c32de"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Est: 10 / Depth: 10 ----- Precision: [0.857 0.833 0.5  ] / Recall: [0.75  0.833 0.667] / Accuracy: 0.765\n",
            "Est: 10 / Depth: 20 ----- Precision: [0.857 1.    0.6  ] / Recall: [0.75  0.833 1.   ] / Accuracy: 0.824\n",
            "Est: 10 / Depth: 30 ----- Precision: [0.857 0.833 0.5  ] / Recall: [0.75  0.833 0.667] / Accuracy: 0.765\n",
            "Est: 10 / Depth: None ----- Precision: [0.857 0.833 0.5  ] / Recall: [0.75  0.833 0.667] / Accuracy: 0.765\n",
            "Est: 50 / Depth: 10 ----- Precision: [0.857 0.833 0.5  ] / Recall: [0.75  0.833 0.667] / Accuracy: 0.765\n",
            "Est: 50 / Depth: 20 ----- Precision: [0.857 0.833 0.5  ] / Recall: [0.75  0.833 0.667] / Accuracy: 0.765\n",
            "Est: 50 / Depth: 30 ----- Precision: [0.857 0.833 0.5  ] / Recall: [0.75  0.833 0.667] / Accuracy: 0.765\n",
            "Est: 50 / Depth: None ----- Precision: [0.857 0.833 0.5  ] / Recall: [0.75  0.833 0.667] / Accuracy: 0.765\n",
            "Est: 100 / Depth: 10 ----- Precision: [0.857 0.833 0.5  ] / Recall: [0.75  0.833 0.667] / Accuracy: 0.765\n",
            "Est: 100 / Depth: 20 ----- Precision: [0.857 0.833 0.5  ] / Recall: [0.75  0.833 0.667] / Accuracy: 0.765\n",
            "Est: 100 / Depth: 30 ----- Precision: [0.857 0.833 0.5  ] / Recall: [0.75  0.833 0.667] / Accuracy: 0.765\n",
            "Est: 100 / Depth: None ----- Precision: [0.857 0.833 0.5  ] / Recall: [0.75  0.833 0.667] / Accuracy: 0.765\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "На момент тестирования алгоритма для имеющегося датасета после est = 10 и depth = 20 улучшений точности алгоритма при увеличении показателей не наблюдается"
      ],
      "metadata": {
        "id": "1jzns1ONHgN7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_est = 10\n",
        "depth = 20"
      ],
      "metadata": {
        "id": "Yvx89FhYIC4K"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gb = GradientBoostingClassifier(n_estimators=n_est, max_depth=depth)\n",
        "gb_model = gb.fit(X_train, y_train)\n",
        "print(gb_model.predict(X_test))"
      ],
      "metadata": {
        "id": "4qIBCOq-H5FD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42c9171a-241e-4e5e-b00f-4c47754a363a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1 0 1 0 0 1 2 2 0 2 0 1 1 0 2 1 0]\n"
          ]
        }
      ]
    }
  ]
}