{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mkkEcZONBTP3",
        "outputId": "06f57ae3-cd85-4a34-9405-e11b9a5f70cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pymorphy2 in /usr/local/lib/python3.10/dist-packages (0.9.1)\n",
            "Requirement already satisfied: dawg-python>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from pymorphy2) (0.7.2)\n",
            "Requirement already satisfied: pymorphy2-dicts-ru<3.0,>=2.4 in /usr/local/lib/python3.10/dist-packages (from pymorphy2) (2.4.417127.4579844)\n",
            "Requirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.10/dist-packages (from pymorphy2) (0.6.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install pymorphy2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import string\n",
        "import re\n",
        "import nltk\n",
        "import pymorphy2\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.metrics import precision_recall_fscore_support as score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer"
      ],
      "metadata": {
        "id": "ErubAXqDCjo3"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.set_option('display.max_colwidth', 100)"
      ],
      "metadata": {
        "id": "MG3MvWMgC65n"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Предобработка"
      ],
      "metadata": {
        "id": "0ZTxvisGC9f8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_punctuation(text):\n",
        "    text_nopunct = \"\".join([char for char in text if char not in string.punctuation])\n",
        "    return text_nopunct\n",
        "\n",
        "def tokenise(text):\n",
        "    tokens = re.split('\\W+', text)\n",
        "    return tokens\n",
        "\n",
        "def remove_stopwords(tokenised_list):\n",
        "    stopwords = nltk.corpus.stopwords.words('russian')\n",
        "    filtered_text = [word for word in tokenised_list if word not in stopwords]\n",
        "    return filtered_text\n",
        "\n",
        "def stemming(tokenised_text):\n",
        "    ps = nltk.SnowballStemmer('russian')\n",
        "    processed_text = [ps.stem(word) for word in tokenised_text]\n",
        "    return processed_text\n",
        "\n",
        "def lemmatizing(tokenized_text):\n",
        "    ma = pymorphy2.MorphAnalyzer()\n",
        "    processed_text = [ma.parse(word)[0].normal_form for word in tokenized_text]\n",
        "    return processed_text"
      ],
      "metadata": {
        "id": "JTGdUvDQDBe5"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    text = remove_punctuation(text)\n",
        "    tokens = tokenise(text)\n",
        "    processed_text = lemmatizing(tokens)\n",
        "    return processed_text"
      ],
      "metadata": {
        "id": "nS-vl3sMGFxm"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Загрузка данных"
      ],
      "metadata": {
        "id": "tGBQ8q_FDS4C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(\"requests_gpt.csv\", encoding='windows-1251', sep = ';')\n",
        "data.columns = ['label', 'requests']"
      ],
      "metadata": {
        "id": "1yKGWBMdDFQ3"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###TF-IDF для векторизации"
      ],
      "metadata": {
        "id": "2A0_Mu7qEBY9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_vect = TfidfVectorizer(analyzer=clean_text)\n",
        "\n",
        "X_tfidf = tfidf_vect.fit_transform(data['requests'])\n",
        "X_features = pd.DataFrame(X_tfidf.toarray())\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_features, data['label'], test_size = 0.3)"
      ],
      "metadata": {
        "id": "-vCbUAh6EBv_"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###GradientBoostingClassifier"
      ],
      "metadata": {
        "id": "hhX5p8arDIZs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Проверка на лучшее сочетание параметров метода GradientBoosting"
      ],
      "metadata": {
        "id": "uM0nOrU8EyWP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_GB (n_est, depth):\n",
        "  gb = GradientBoostingClassifier(n_estimators=n_est, max_depth=depth)\n",
        "  gb_model = gb.fit(X_train, y_train)\n",
        "  y_pred = gb_model.predict(X_test)\n",
        "\n",
        "  precision, recall, fscore, support = score(y_test, y_pred)\n",
        "  print('Est: {} / Depth: {} ----- Precision: {} / Recall: {} / Accuracy: {}'.format(\n",
        "        n_est, depth,\n",
        "        np.round(precision, 3), np.round(recall,3), round((y_pred==y_test).sum() / len(y_pred), 3)))"
      ],
      "metadata": {
        "id": "dJfQH9xOEoon"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for n_est in [10, 50, 100]:\n",
        "    for depth in [10, 20, 30, None]:\n",
        "        test_GB(n_est, depth)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-OVjFUgqF6Rc",
        "outputId": "09da727e-ebf2-40df-da31-01dea55df601"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Est: 10 / Depth: 10 ----- Precision: [0.625 0.75  1.   ] / Recall: [1.    0.6   0.714] / Accuracy: 0.765\n",
            "Est: 10 / Depth: 20 ----- Precision: [0.714 0.8   1.   ] / Recall: [1.    0.8   0.714] / Accuracy: 0.824\n",
            "Est: 10 / Depth: 30 ----- Precision: [0.714 0.8   1.   ] / Recall: [1.    0.8   0.714] / Accuracy: 0.824\n",
            "Est: 10 / Depth: None ----- Precision: [0.714 0.8   1.   ] / Recall: [1.    0.8   0.714] / Accuracy: 0.824\n",
            "Est: 50 / Depth: 10 ----- Precision: [0.714 0.8   1.   ] / Recall: [1.    0.8   0.714] / Accuracy: 0.824\n",
            "Est: 50 / Depth: 20 ----- Precision: [0.714 0.8   1.   ] / Recall: [1.    0.8   0.714] / Accuracy: 0.824\n",
            "Est: 50 / Depth: 30 ----- Precision: [0.714 0.8   1.   ] / Recall: [1.    0.8   0.714] / Accuracy: 0.824\n",
            "Est: 50 / Depth: None ----- Precision: [0.714 0.8   1.   ] / Recall: [1.    0.8   0.714] / Accuracy: 0.824\n",
            "Est: 100 / Depth: 10 ----- Precision: [0.714 0.8   1.   ] / Recall: [1.    0.8   0.714] / Accuracy: 0.824\n",
            "Est: 100 / Depth: 20 ----- Precision: [0.714 0.8   1.   ] / Recall: [1.    0.8   0.714] / Accuracy: 0.824\n",
            "Est: 100 / Depth: 30 ----- Precision: [0.714 0.8   1.   ] / Recall: [1.    0.8   0.714] / Accuracy: 0.824\n",
            "Est: 100 / Depth: None ----- Precision: [0.714 0.8   1.   ] / Recall: [1.    0.8   0.714] / Accuracy: 0.824\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "На момент тестирования алгоритма для имеющегося датасета после est = 10 и depth = 20 улучшений точности алгоритма при увеличении показателей не наблюдается"
      ],
      "metadata": {
        "id": "1jzns1ONHgN7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_est = 10\n",
        "depth = 20"
      ],
      "metadata": {
        "id": "Yvx89FhYIC4K"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gb = GradientBoostingClassifier(n_estimators=n_est, max_depth=depth)\n",
        "gb_model = gb.fit(X_train, y_train)\n",
        "#print(gb_model.predict(X_test))"
      ],
      "metadata": {
        "id": "4qIBCOq-H5FD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}