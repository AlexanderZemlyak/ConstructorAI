{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GG-VsSODLwf9",
        "outputId": "dcabe517-da3a-4bb4-ba60-4ccb470cc253"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pymorphy2\n",
            "  Downloading pymorphy2-0.9.1-py3-none-any.whl (55 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.5/55.5 kB\u001b[0m \u001b[31m573.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dawg-python>=0.7.1 (from pymorphy2)\n",
            "  Downloading DAWG_Python-0.7.2-py2.py3-none-any.whl (11 kB)\n",
            "Collecting pymorphy2-dicts-ru<3.0,>=2.4 (from pymorphy2)\n",
            "  Downloading pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl (8.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docopt>=0.6 (from pymorphy2)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: docopt\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=5286328988f9a58aaa8ac5962e01d2781b844a9cd7bfd073648b24931308a3a0\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
            "Successfully built docopt\n",
            "Installing collected packages: pymorphy2-dicts-ru, docopt, dawg-python, pymorphy2\n",
            "Successfully installed dawg-python-0.7.2 docopt-0.6.2 pymorphy2-0.9.1 pymorphy2-dicts-ru-2.4.417127.4579844\n"
          ]
        }
      ],
      "source": [
        "!pip install pymorphy2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "9ebSJWURLwf_"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import string\n",
        "import re\n",
        "import nltk\n",
        "import pymorphy2\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import precision_recall_fscore_support as score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "lrF7PlnxLwf_"
      },
      "outputs": [],
      "source": [
        "pd.set_option('display.max_colwidth', 100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "En4D1PY2Lwf_"
      },
      "source": [
        "Методы предобработки"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Dna94Dl1LwgB"
      },
      "outputs": [],
      "source": [
        "def remove_punctuation(text):\n",
        "    text_nopunct = \"\".join([char for char in text if char not in string.punctuation])\n",
        "    return text_nopunct\n",
        "\n",
        "def tokenise(text):\n",
        "    tokens = re.split('\\W+', text)\n",
        "    return tokens\n",
        "\n",
        "def remove_stopwords(tokenised_list):\n",
        "    stopwords = nltk.corpus.stopwords.words('russian')\n",
        "    filtered_text = [word for word in tokenised_list if word not in stopwords]\n",
        "    return filtered_text\n",
        "\n",
        "def stemming(tokenised_text):\n",
        "    ps = nltk.SnowballStemmer('russian')\n",
        "    processed_text = [ps.stem(word) for word in tokenised_text]\n",
        "    return processed_text\n",
        "\n",
        "def lemmatizing(tokenized_text):\n",
        "    ma = pymorphy2.MorphAnalyzer()\n",
        "    processed_text = [ma.parse(word)[0].normal_form for word in tokenized_text]\n",
        "    return processed_text\n",
        "\n",
        "def clean_text(text):\n",
        "    text = remove_punctuation(text)\n",
        "    tokens = tokenise(text)\n",
        "    processed_text = lemmatizing(tokens)\n",
        "    return processed_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "waN3WvoQLwgC"
      },
      "source": [
        "Тестирование метода random forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "tWKcFpuyLwgC"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv(\"requests_gpt.csv\", encoding='windows-1251', sep = ';')\n",
        "data.columns = ['label', 'requests']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "9qU1rCJ4LwgC"
      },
      "outputs": [],
      "source": [
        "# TF-IDF для векторизации\n",
        "tfidf_vect = TfidfVectorizer(analyzer=clean_text)\n",
        "X_tfidf = tfidf_vect.fit_transform(data['requests'])\n",
        "\n",
        "X_features = pd.DataFrame(X_tfidf.toarray())\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_features, data['label'], test_size = 0.3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GyHFJe80LwgD",
        "outputId": "7262f1a0-2bc3-48b4-8b85-5196331a3b5c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Est: 10 / Depth: 10 ----- Precision: [0.429 0.857 1.   ] / Recall: [1.   0.75 0.5 ] / Accuracy: 0.706\n",
            "Est: 10 / Depth: 20 ----- Precision: [0.286 0.667 0.75 ] / Recall: [0.667 0.5   0.5  ] / Accuracy: 0.529\n",
            "Est: 10 / Depth: 30 ----- Precision: [0.5   1.    0.833] / Recall: [1.    0.625 0.833] / Accuracy: 0.765\n",
            "Est: 10 / Depth: None ----- Precision: [0.375 0.8   1.   ] / Recall: [1.    0.5   0.667] / Accuracy: 0.647\n",
            "Est: 50 / Depth: 10 ----- Precision: [0.429 1.    1.   ] / Recall: [1.    0.625 0.833] / Accuracy: 0.765\n",
            "Est: 50 / Depth: 20 ----- Precision: [0.6   0.875 1.   ] / Recall: [1.    0.875 0.667] / Accuracy: 0.824\n",
            "Est: 50 / Depth: 30 ----- Precision: [0.5 1.  1. ] / Recall: [1.    0.75  0.833] / Accuracy: 0.824\n",
            "Est: 50 / Depth: None ----- Precision: [0.429 0.833 1.   ] / Recall: [1.    0.625 0.667] / Accuracy: 0.706\n",
            "Est: 100 / Depth: 10 ----- Precision: [0.5   0.857 1.   ] / Recall: [1.    0.75  0.667] / Accuracy: 0.765\n",
            "Est: 100 / Depth: 20 ----- Precision: [0.5 1.  1. ] / Recall: [1.    0.75  0.833] / Accuracy: 0.824\n",
            "Est: 100 / Depth: 30 ----- Precision: [0.429 1.    1.   ] / Recall: [1.    0.625 0.833] / Accuracy: 0.765\n",
            "Est: 100 / Depth: None ----- Precision: [0.5 1.  1. ] / Recall: [1.    0.75  0.833] / Accuracy: 0.824\n"
          ]
        }
      ],
      "source": [
        "# вывод характеристик для различных гиперпараметров\n",
        "def train_RF(n_est, depth):\n",
        "    rf = RandomForestClassifier(n_estimators = n_est, max_depth=depth, n_jobs=-1)\n",
        "    rf_model = rf.fit(X_train, y_train)\n",
        "    y_pred = rf_model.predict(X_test)\n",
        "    precision, recall, fscore, support = score(y_test, y_pred)\n",
        "    print('Est: {} / Depth: {} ----- Precision: {} / Recall: {} / Accuracy: {}'.format(\n",
        "        n_est, depth, np.round(precision, 3), np.round(recall,3),\n",
        "        round((y_pred==y_test).sum() / len(y_pred), 3)))\n",
        "\n",
        "for n_est in [10, 50, 100]:\n",
        "    for depth in [10, 20, 30, None]:\n",
        "        train_RF(n_est, depth)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZoO9Is_qLwgD",
        "outputId": "acb80dc5-0940-47b7-a8e5-2145c9273fe2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1 1 2 1 0 2 2 1 0 1 2 0 1 2 0 0 0]\n"
          ]
        }
      ],
      "source": [
        "# работа с лучшим вариантом\n",
        "rf = RandomForestClassifier(n_estimators = 50, max_depth=20, n_jobs=-1)\n",
        "rf_model = rf.fit(X_train, y_train)\n",
        "print(rf_model.predict(X_test))"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}